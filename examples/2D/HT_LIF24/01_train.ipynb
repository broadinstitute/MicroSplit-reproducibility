{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** Training a <nobr>Micro$\\mathbb{S}$plit</nobr> Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - what does this notebook do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Despite network training arguably being the most important step, the execution of this notebook is optional. If you do not run this notebook, in Step 2 (prediction) we offer you to use pretrained model checkpoints. Like in any respected cooking show: \"we have already prepared someting before the show\"... üòâ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will train a <nobr>Micro$\\mathbb{S}$plit</nobr> network for unmixing two superimposed channels from our HT-LIF24 dataset. \n",
    "\n",
    "This dataset contains four labeled structures, and soon you will be asked to choose which of these two you want to learn to unmix: \n",
    "1. Cell Nucleui,\n",
    "1. Microtubules,\n",
    "1. Nuclear Membrane, and\n",
    "1. Centromeres/Kinetocores.\n",
    "\n",
    "Additionally to four total structures, this dataset also offers acquisitions taken with different exposure times, hence, the data is available at various SNR, i.e. [signal-to-noise ratios](https://en.wikipedia.org/wiki/Signal-to-noise_ratio#:~:text=Signal%2Dto%2Dnoise%20ratio%20(,power%2C%20often%20expressed%20in%20decibels.). Shorter exposure times lead to a collection of fewer photos, which leads to more noise and therefore a lower SNR.\n",
    "\n",
    "The lower the SNR of the data you will choose to train <nobr>Micro$\\mathbb{S}$plit</nobr> with, the more important will the unsupervised denoising feature of <nobr>Micro$\\mathbb{S}$plit</nobr> become.\n",
    "\n",
    "But enough introduction, let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: what is <nobr>Micro$\\mathbb{S}$plit</nobr> training all about?\n",
    "Training is done in a supervised way. For every input patch, we have the two corresponding target patches using which we train our MicroSplit. \n",
    "Besides the primary input patch, we also feed LC inputs to MicroSplit. We introduced LC inputs in [ŒºSplit: efficient image decomposition for microscopy data](https://openaccess.thecvf.com/content/ICCV2023/papers/Ashesh_uSplit_Image_Decomposition_for_Fluorescence_Microscopy_ICCV_2023_paper.pdf), which enabled the network to understand the global spatial context around the input patch.\n",
    "\n",
    "To enable unsupervised denoising, we integrated the KL loss formulation and Noise models from our previous work [denoiSplit: a method for joint microscopy image splitting and unsupervised denoising](https://eccv.ecva.net/virtual/2024/poster/2538). \n",
    "\n",
    "The loss function for MicroSplit is a weighted average of denoiSplit loss and ŒºSplit loss. For both denoiSplit and ŒºSplit, their loss expression have two terms: KL divergence loss and likelihood loss. For more details, please refer to the respective papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it, let's train a <nobr>Micro$\\mathbb{S}$plit</nobr> Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are new to Jupyter notebooks?** Don't worry, if you take the time to read all our explanations, we will guide you through them and you will understand a lot. Still, you will likely end up less frustrated, if you do not even start with the ambition to interpret the purpose of every line of code.\n",
    "Let's start with a nice example, the imports to enable the remainder of this notebook. Ignore it (unless you know what you are doing) and just click **‚áß*Shift* + ‚èé*Enter*** to execute this (and all other) code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things we need further down\n",
    "\n",
    "import pooch\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from careamics.lightning import VAEModule\n",
    "\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint, load_checkpoint_path\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.utils import (\n",
    "    plot_training_metrics,\n",
    "    plot_input_patches,\n",
    "    plot_training_outputs,\n",
    ")\n",
    "\n",
    "# Dataset specific imports...\n",
    "from microsplit_reproducibility.configs.parameters.HT_LIF24 import get_microsplit_parameters\n",
    "from microsplit_reproducibility.configs.data.HT_LIF24 import get_data_configs\n",
    "from microsplit_reproducibility.datasets.HT_LIF24 import get_train_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.1:** Data Preparation\n",
    "\n",
    "Without any special modifications, **the following few steps will first choose which subset of the *HT_LIF24* dataset to use and then train a <nobr>Micro$\\mathbb{S}$plit</nobr> Model on this data**.\n",
    "\n",
    "The reason to pick a subset of the *HT_LIF24* dataset is, that we recorded this data with multiple possible learning tasks in mind. Check the <nobr>Micro$\\mathbb{S}$plit</nobr> paper for all the details.\n",
    "\n",
    "This logic you find below will likely also apply to your needs when training on your own data (in case this is what you are after). Still, don't shy away from deviating from the code you find below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the 2, 3 or 4 channels you want train on, and the exposure time (SNR) of these channel acquisitions you want to use...\n",
    "To make things more human readable, we use the utility function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line is likely a bit confusing at first.\n",
    "\n",
    "Since the channel unmixing capabilities of <nobr>Micro$\\mathbb{S}$plit</nobr> are trained in a supervised way, we must later feed *(i)* input images that contain certain number of selected structures, and *(ii)* corresponding number of separate channels that show these structures separately.\n",
    "\n",
    "The following code cell will help you to select the channels and exposure time you want to train on.\n",
    "Exposure duration is a parameter of this particular experience and is defined in seconds.\n",
    "\n",
    "    VeryLow = \"2ms\"\n",
    "    Low = \"3ms\"\n",
    "    Medium = \"5ms\"\n",
    "    High = \"20ms\"\n",
    "    VeryHigh = \"500ms\"\n",
    "\n",
    "Names of channels follow the following convention:\n",
    "\n",
    "    Nucleus = 0\n",
    "    MicroTubules = 1\n",
    "    NuclearMembrane = 2\n",
    "    Centromere = 3\n",
    "    Input = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen structures: [1, 2]\n",
      "All data channels to be loaded: [1, 2, 11]\n"
     ]
    }
   ],
   "source": [
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import ExposureDuration, define_experiment_config\n",
    "\n",
    "CHANNEL_IDX_LIST, TARGET_CHANNEL_IDX_LIST, EXPOSURE_DURATION = define_experiment_config(\n",
    "    num_channels=2, exposure=ExposureDuration.Medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data you need (given your choices from above)\n",
    "Depending on your internet connection, this will take a while...\n",
    "\n",
    "Note that appropriate noise models will only be downloaded if they were not created by executing the notebook `00_noisemodels.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pooch.create(\n",
    "    path=\"./data/\",\n",
    "    base_url=\"https://download.fht.org/jug/msplit/ht_lif24/data/\",\n",
    "    registry={f\"ht_lif24_{EXPOSURE_DURATION}.zip\": None},\n",
    ")\n",
    "\n",
    "NOISE_MODELS = pooch.create(\n",
    "    path=f\"./noise_models/{EXPOSURE_DURATION}/\",\n",
    "    base_url=f\"https://download.fht.org/jug/msplit/ht_lif24/noise_models/{EXPOSURE_DURATION}/\",\n",
    "    registry={\n",
    "        f\"noise_model_Ch{channel_idx}.npz\": None\n",
    "        for channel_idx in TARGET_CHANNEL_IDX_LIST\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following notebook will download all you need. In case you see no messages announcing any downloads, you already have this data in the notebook folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in NOISE_MODELS.registry:\n",
    "    NOISE_MODELS.fetch(fname, progressbar=True)\n",
    "print('---------')\n",
    "for fname in DATA.registry:\n",
    "    DATA.fetch(fname, processor=pooch.Unzip(), progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we load the image data to be processed\n",
    "\n",
    "Note that depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train, validation, and test data configs\n",
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    dset_type=EXPOSURE_DURATION, channel_idx_list=CHANNEL_IDX_LIST,\n",
    ")\n",
    "# setting up MicroSplit parametrization\n",
    "experiment_params = get_microsplit_parameters(\n",
    "    dset_type=EXPOSURE_DURATION, nm_path=NOISE_MODELS.path, channel_idx_list=CHANNEL_IDX_LIST, batch_size=32\n",
    ")\n",
    "\n",
    "# start the download of required files\n",
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=DATA.path / f\"ht_lif24_{EXPOSURE_DURATION}.zip.unzip/{EXPOSURE_DURATION}\",\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=get_train_val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optional:*** inspect data configurations and <nobr>Micro$\\mathbb{S}$plit</nobr> config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_show_configs = True\n",
    "\n",
    "if do_show_configs:\n",
    "    print('FYI: train_data_config')\n",
    "    print('----------------------')\n",
    "    for cfg in train_data_config:\n",
    "        print(cfg)\n",
    "\n",
    "    print('\\nFYI: experiment_params')\n",
    "    print('----------------------')\n",
    "    print(experiment_params)\n",
    "else:\n",
    "    print('You opted out of having all params printed... swiftly moving on... ;)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wanna trade speed for model quality? (1/2)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you just want to get an idea of the process of training <nobr>Micro$\\mathbb{S}$plit</nobr> and you do not intend to get best-possible results, feel invited to crop down on the training data to be used further down. <i><b>Do not do this</b> if you intend to train a competitive model!!!</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, training and validation data will be reduced to only \n",
    "# consisting of 2 and 1 frames, respectively.\n",
    "reduce_data = True\n",
    "\n",
    "if reduce_data:\n",
    "    print(\"Using REDUCED training and validation data for quick'n'dirty testing!\")\n",
    "    train_dset.reduce_data(list(range(10)))\n",
    "    val_dset.reduce_data([0])\n",
    "else:\n",
    "    print('Using the full set of training and validation data!')\n",
    "print(f'(This are {train_dset.get_num_frames()} and {val_dset.get_num_frames()} frames, respectively.)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: create Dataloaders for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.2:** Prepare <nobr>Micro$\\mathbb{S}$plit</nobr> Training\n",
    "Next, we create all the configs for the upcoming network training run. These lines are not very intuitive and if you don't intend to dive really deep into CAREamics and the internals of <nobr>Micro$\\mathbb{S}$plit</nobr>, you might just execute these cells and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our data_stas known to the experiment we prepare\n",
    "experiment_params[\"data_stats\"] = data_stats\n",
    "\n",
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wanna trade speed for model quality? (2/2)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Similar to above, if you simply want to go over this notebook quickly and you don't care about training a competitive model, set <i>`reduce_training_epochs=True`</i> in the cell below. \n",
    "\n",
    "Note that if <i> reduce_training_epochs </i> is set to <i>False </i>, the default number of epochs is 400. However, early stopping will be invoked when the loss is no longer decreasing, so the actual number of epochs trained will most likely be less than 400.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_training_epochs = True\n",
    "\n",
    "# if True, train for only 10 epochs (quick'n'dirty testing mode)\n",
    "if reduce_training_epochs:\n",
    "    training_config.num_epochs = 10\n",
    "\n",
    "print(f'Will train for {training_config.num_epochs} epochs!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the <nobr>Micro$\\mathbb{S}$plit</nobr> model to be trained.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load checkpoint (optional and for you to implement)*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> If you would like to continue a previous training run or finetune a compatible pre-trained model, here would be a good place. You will need to figure out how to implement this for your use-case, but to give you a head-start, we left three potentially useful lines of code below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from microsplit_reproducibility.notebook_utils.HT_LIF24 import load_pretrained_model\n",
    "# ckpt_path = load_checkpoint_path(f\"./pretrained_checkpoints/{EXPOSURE_DURATION}/\", best=True)\n",
    "# load_pretrained_model(model, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some training data for a final check!\n",
    "***Tip:*** we show you a few samples of the prepared training data. In case you don't like what you see, execute the cell again and other randomly chosen patches will be shown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_patches(dataset=train_dset, num_channels=len(TARGET_CHANNEL_IDX_LIST), num_samples=3, patch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.3:** Train the prepared model!\n",
    "***Note:*** if this takes too long, there were to places above where we gave you options to *(i)* reduce the amount of training data, and *(ii)* chose to train for fewer epochs. Revisit your choices if you want to!\n",
    "\n",
    "***Note:*** Depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CAREamics 'Trainer'\n",
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    # NOTE: if you are on a mac swap the accelerator to \"mps\"\n",
    "    # accelerator=‚Äúmps‚Äù,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(f\"./checkpoints/{EXPOSURE_DURATION}\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "# start the training - yay!\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training loss curves...\n",
    "Below, we plot for each epoch of your training run the *(i)* training reconstruction loss, *(ii)* training KL divergence loss, *(iii)* validation reconstruction loss, and *(iv)* validation PSNR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import find_recent_metrics, plot_metrics\n",
    "df = read_csv(find_recent_metrics())\n",
    "plot_metrics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.4:** Predict and visualize results for validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params['mmse_count'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import get_unnormalized_predictions, get_target, get_input\n",
    "stitched_predictions, _, _ = get_unnormalized_predictions(model, val_dset, EXPOSURE_DURATION, TARGET_CHANNEL_IDX_LIST, \n",
    "                                                    mmse_count = experiment_params['mmse_count'], num_workers=4, batch_size=32)\n",
    "tar = get_target(val_dset)\n",
    "inp = get_input(val_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: visualize predictions on validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import full_frame_evaluation\n",
    "frame_idx = 0\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds\"\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed view on some (foreground) locations...\n",
    "Below, we show few random foreground locations and the corresponding <nobr>Micro$\\mathbb{S}$plit</nobr> predictions.\n",
    "\n",
    "As before, also here you can execute the cell multiple times and different randomly chosen locations will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from microsplit_reproducibility.utils.utils import clean_ax\n",
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import pick_random_patches_with_content\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sz = 128\n",
    "rand_locations = pick_random_patches_with_content(tar, 128)\n",
    "h_start = rand_locations[2,0] #np.random.randint(stitched_predictions.shape[1] - img_sz)\n",
    "w_start = rand_locations[2,1] #np.random.randint(stitched_predictions.shape[2] - img_sz)\n",
    "\n",
    "ncols = 2*len(TARGET_CHANNEL_IDX_LIST) + 1\n",
    "nrows = min(len(rand_locations), 5)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "\n",
    "for i, (h_start, w_start) in enumerate(rand_locations[:nrows]):\n",
    "    ax[i,0].imshow(inp[0,h_start:h_start+img_sz, w_start:w_start+img_sz])\n",
    "    for j in range(ncols//2):\n",
    "        vmin = stitched_predictions[0,h_start:h_start+img_sz, w_start:w_start+img_sz,j].min()\n",
    "        vmax = stitched_predictions[0,h_start:h_start+img_sz, w_start:w_start+img_sz,j].max()\n",
    "        ax[i,2*j+1].imshow(tar[0,h_start:h_start+img_sz, w_start:w_start+img_sz,j], vmin=vmin, vmax=vmax)\n",
    "        ax[i,2*j+2].imshow(stitched_predictions[0,h_start:h_start+img_sz, w_start:w_start+img_sz,j], vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax[0,0].set_title('Primary Input')\n",
    "for i in range(len(TARGET_CHANNEL_IDX_LIST)):\n",
    "    ax[0,2*i+1].set_title(f'Target Channel {i+1}')\n",
    "    ax[0,2*i+2].set_title(f'Predicted Channel {i+1}')\n",
    "\n",
    "# reduce the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
    "clean_ax(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* manual inspection of the predictions\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Task:</b> Set <i>y_start</i>, <i>x_start</i>, and <i>crop_size</i> to inspect the predictions at a  location of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_start = 600  #np.random.randint(stitched_predictions.shape[1] - crop_size)\n",
    "x_start = 1150 #np.random.randint(stitched_predictions.shape[2] - crop_size)\n",
    "crop_size = 128\n",
    "\n",
    "ncols = len(TARGET_CHANNEL_IDX_LIST) + 1\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "ax[0,0].imshow(inp[0,y_start:y_start+crop_size, x_start:x_start+crop_size])\n",
    "for i in range(ncols -1):\n",
    "    vmin = stitched_predictions[0,y_start:y_start+crop_size, x_start:x_start+crop_size,i].min()\n",
    "    vmax = stitched_predictions[0,y_start:y_start+crop_size, x_start:x_start+crop_size,i].max()\n",
    "    ax[0,i+1].imshow(tar[0,y_start:y_start+crop_size, x_start:x_start+crop_size,i], vmin=vmin, vmax=vmax)\n",
    "    ax[0,i+1].set_title(f\"Channel {i+1}\")\n",
    "    ax[1,i+1].imshow(stitched_predictions[0,y_start:y_start+crop_size, x_start:x_start+crop_size,i], vmin=vmin, vmax=vmax)\n",
    "\n",
    "# disable the axis for ax[1,0]\n",
    "ax[0,0].set_title(\"Input\")\n",
    "\n",
    "print('Here the crop you selected:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Optional Step 1.4:*** Posterior Sampling\n",
    "For a given input patch, <nobr>Micro$\\mathbb{S}$plit</nobr> can generate multiple outputs. This is possible because <nobr>Micro$\\mathbb{S}$plit</nobr> is learning a full posterior of possible solutions, which is a quite powerful feature!\n",
    "\n",
    "As we elaborate in the <nobr>Micro$\\mathbb{S}$plit</nobr> paper and also later in the calibration notebook `03_calibration.ipynb`, this allows users to visually judge and even quantify the (data) uncertainty in the predictions their trained model makes.\n",
    "\n",
    "Below, we show two posterior samples and how much they differ for a few random foreground locations. Re-run the cell to see different randomly choosen locations and corresponding posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import show_sampling\n",
    "imgsz = 3\n",
    "ncols = 6\n",
    "examplecount = 3\n",
    "_,ax = plt.subplots(figsize=(imgsz*ncols, imgsz*2*examplecount), ncols=ncols, nrows=2*examplecount)\n",
    "\n",
    "show_sampling(val_dset, model, ax=ax[:2])\n",
    "show_sampling(val_dset, model, ax=ax[2:4])\n",
    "show_sampling(val_dset, model, ax=ax[4:6])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are done here! üëç Congratulations! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
