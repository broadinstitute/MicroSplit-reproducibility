{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** Training a <nobr>Micro$\\mathbb{S}$plit</nobr> Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - what does this notebook do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will train a <nobr>Micro$\\mathbb{S}$plit</nobr> network for unmixing two superimposed channels for a custom dataset you provide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> How to organize your data for training a <nobr>Micro$\\mathbb{S}$plit</nobr> model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will train a <nobr>Micro$\\mathbb{S}$plit</nobr> network for unmixing two or more superimposed channels for a custom 2D dataset you provide. \n",
    "\n",
    "You should organize you dataset as follows:\n",
    "- Create a `data` directory\n",
    "- Create subdirectories `channel_1`, `channel_2`, etc, containing the channels you would like to unmix\n",
    "- Make sure that the images have the same spatial size and each image has only 1 channel\n",
    "\n",
    "Your data directory should look like this:\n",
    "```\n",
    "you_data_path/\n",
    "‚îî‚îÄ‚îÄ data\n",
    "    ‚îú‚îÄ‚îÄ channel_1\n",
    "    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ image1.tiff\n",
    "    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ image2.tiff\n",
    "    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ image3.tiff\n",
    "    ‚îî‚îÄ‚îÄ channel_2\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image1.tiff\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image2.tiff\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ image3.tiff\n",
    "    ‚îî‚îÄ‚îÄ channel_n\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image1.tiff\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image2.tiff\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ image3.tiff\n",
    "```\n",
    "\n",
    "The mixed image used for splitting will be obtained artificially by a convex combination of the target channels.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: what is <nobr>Micro$\\mathbb{S}$plit</nobr> training all about?\n",
    "Training is done in a supervised way. For every input patch, we have the two corresponding target patches using which we train our MicroSplit. \n",
    "Besides the primary input patch, we also feed LC inputs to MicroSplit. We introduced LC inputs in [ŒºSplit: efficient image decomposition for microscopy data](https://openaccess.thecvf.com/content/ICCV2023/papers/Ashesh_uSplit_Image_Decomposition_for_Fluorescence_Microscopy_ICCV_2023_paper.pdf), which enabled the network to understand the global spatial context around the input patch.\n",
    "\n",
    "To enable unsupervised denoising, we integrated the KL loss formulation and Noise models from our previous work [denoiSplit: a method for joint microscopy image splitting and unsupervised denoising](https://eccv.ecva.net/virtual/2024/poster/2538). \n",
    "\n",
    "The loss function for MicroSplit is a weighted average of denoiSplit loss and ŒºSplit loss. For both denoiSplit and ŒºSplit, their loss expression have two terms: KL divergence loss and likelihood loss. For more details, please refer to the respective papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it, let's train a <nobr>Micro$\\mathbb{S}$plit</nobr> Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are new to Jupyter notebooks?** Don't worry, if you take the time to read all our explanations, we will guide you through them and you will understand a lot. Still, you will likely end up less frustrated, if you do not even start with the ambition to interpret the purpose of every line of code.\n",
    "Let's start with a nice example, the imports to enable the remainder of this notebook. Ignore it (unless you know what you are doing) and just click **‚áß*Shift* + ‚èé*Enter*** to execute this (and all other) code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things we need further down\n",
    "from pathlib import Path\n",
    "\n",
    "import pooch\n",
    "import matplotlib.pyplot as plt\n",
    "from careamics.lightning import VAEModule\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint_path\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.utils import plot_input_patches\n",
    "\n",
    "# Dataset specific imports...\n",
    "from microsplit_reproducibility.configs.parameters.custom_dataset_2D import (\n",
    "    get_microsplit_parameters\n",
    ")\n",
    "from microsplit_reproducibility.configs.data.custom_dataset_2D import get_data_configs\n",
    "from microsplit_reproducibility.datasets.custom_dataset_2D import get_train_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.1:** Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the channel unmixing capabilities of <nobr>Micro$\\mathbb{S}$plit</nobr> are trained in a supervised way, we must later feed *(i)* input images that contain both selected structures, and *(ii)* two seperate channels that show these two structures separately. As previosuly mentioned, the mixed input image is obtained synthetically by overlapping the other two channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pooch.create(\n",
    "    path=f\"./data/\",\n",
    "    base_url=f\"https://download.fht.org/jug/msplit/ht_lif24/data_tiff/\",\n",
    "    registry={f\"ht_lif24_5ms_reduced.zip\": None},\n",
    ")\n",
    "for fname in DATA.registry:\n",
    "    DATA.fetch(fname, processor=pooch.Unzip(), progressbar=True)\n",
    "\n",
    "DATA_PATH = DATA.abspath / (DATA.registry_files[0] + \".unzip/5ms/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR set the path to your own data\n",
    "Important: the path should end with `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/my/path/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the path to the noise models\n",
    "This is the path to the noise models that you trained in the notebook **00_noisemodels.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NM_PATH = Path(\"./noise_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we load the image data to be processed\n",
    "\n",
    "***Note*** that depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> below.\n",
    "\n",
    "Number of epochs is set to 10, which usually allows to see decent results. However, for getting optimal performance you can increase it to 50.\n",
    "\n",
    "You also need to change the `num_channels` and `target_channels` parameters respectively for the `get_data_configs()` and the `get_microsplit_parameters` functions. These parameters have a similar meaning, i.e., they control the number of channels in the input data depending on how many channels you want to split.\n",
    "\n",
    "Finally, ensure that the `image_size` parameter (i.e., the patch size in (`Z`, `Y`, `X`) you want to use to train the model) is properly set given the size of your data and of you GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 2\n",
    "\"\"\"The number of channels considered for the splitting task.\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "\"\"\"The batch size for training.\"\"\"\n",
    "PATCH_SIZE = (64, 64)\n",
    "\"\"\"The size of the patches fed to the network for training in (Y, X).\"\"\"\n",
    "EPOCHS = 10\n",
    "\"\"\"The number of epochs to train the network.\"\"\"\n",
    "\n",
    "assert len(PATCH_SIZE) == 2, \"PATCH_SIZE must be a tuple of length 2 (Y, X) since we are using 2D data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding is not used with this alignement style\n",
      "\n",
      "Padding is not used with this alignement style\n",
      "\n",
      "Padding is not used with this alignement style\n"
     ]
    }
   ],
   "source": [
    "# setting up train, validation, and test data configs\n",
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    image_size=PATCH_SIZE,\n",
    "    num_channels=NUM_CHANNELS\n",
    ")\n",
    "\n",
    "# setting up MicroSplit parametrization\n",
    "experiment_params = get_microsplit_parameters(\n",
    "    algorithm=\"denoisplit\",\n",
    "    img_size=PATCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    multiscale_count=3,\n",
    "    noise_model_path=NM_PATH,\n",
    "    target_channels=NUM_CHANNELS,\n",
    ")\n",
    "\n",
    "# create the dataset\n",
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=DATA_PATH,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=get_train_val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/igor.zubarev/projects/microSplit-reproducibility/examples/2D/custom_dataset/data/ht_lif24_5ms_reduced.zip.unzip/5ms/data')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset.get_idx_manager().get_individual_dim_grid_count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1608, 1608, 2), (20, 1608, 1608, 2))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset._data.shape, val_dset._data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset.get_idx_manager().grid_count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset.get_idx_manager().total_grid_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "50\n",
      "1\n",
      "1\n",
      "2500\n",
      "50\n",
      "1\n",
      "1\n",
      "800\n",
      "2500\n",
      "50\n",
      "2500\n",
      "2500\n",
      "1\n",
      "2500\n",
      "50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m train_dset:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/careamics/src/careamics/lvae_training/dataset/lc_dataset.py:206\u001b[0m, in \u001b[0;36mLCMultiChDloader.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: Union[\u001b[38;5;28mint\u001b[39m, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]):\n\u001b[0;32m--> 206\u001b[0m     img_tuples, noise_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uncorrelated_channels:\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    209\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    210\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUncorrelated channels is not implemented when there is a separate input channel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/projects/careamics/src/careamics/lvae_training/dataset/lc_dataset.py:152\u001b[0m, in \u001b[0;36mLCMultiChDloader._get_img\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mReturns the primary patch along with low resolution patches centered on the primary patch.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Noise_tuples is populated when there is synthetic noise in training\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Should have similar type of noise with the noise model\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Starting with microsplit, dump the noise, use it instead as an augmentation if nessesary\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m img_tuples, noise_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_img_sz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m h, w \u001b[38;5;241m=\u001b[39m img_tuples[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/projects/careamics/src/careamics/lvae_training/dataset/multich_dataset.py:624\u001b[0m, in \u001b[0;36mMultiChDloader._load_img\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     idx \u001b[38;5;241m=\u001b[39m index[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 624\u001b[0m patch_loc_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_patch_location_from_dataset_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[patch_loc_list[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# if self._5Ddata:\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m#     assert self._noise_data is None, 'Noise is not supported for 5D data'\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m#     n_loc, z_loc = patch_loc_list[:2]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m#     imgs = self._data[patch_loc_list[0]]\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/careamics/src/careamics/lvae_training/dataset/utils/index_manager.py:133\u001b[0m, in \u001b[0;36mGridIndexManager.get_patch_location_from_dataset_idx\u001b[0;34m(self, dataset_idx)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_patch_location_from_dataset_idx\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Returns the patch location of the grid in the dataset.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     grid_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_location_from_dataset_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_offset()\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(grid_location) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(offset))\n",
      "File \u001b[0;32m~/projects/careamics/src/careamics/lvae_training/dataset/utils/index_manager.py:183\u001b[0m, in \u001b[0;36mGridIndexManager.get_location_from_dataset_idx\u001b[0;34m(self, dataset_idx)\u001b[0m\n\u001b[1;32m    181\u001b[0m     grid_idx\u001b[38;5;241m.\u001b[39mappend(dataset_idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_count(dim))\n\u001b[1;32m    182\u001b[0m     dataset_idx \u001b[38;5;241m=\u001b[39m dataset_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_count(dim)\n\u001b[0;32m--> 183\u001b[0m location \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gridstart_location_from_dim_index(dim, grid_idx[dim])\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_shape))\n\u001b[1;32m    186\u001b[0m ]\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(location)\n",
      "File \u001b[0;32m~/projects/careamics/src/careamics/lvae_training/dataset/utils/index_manager.py:183\u001b[0m, in \u001b[0;36mGridIndexManager.get_location_from_dataset_idx\u001b[0;34m(self, dataset_idx)\u001b[0m\n\u001b[1;32m    181\u001b[0m     grid_idx\u001b[38;5;241m.\u001b[39mappend(dataset_idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_count(dim))\n\u001b[1;32m    182\u001b[0m     dataset_idx \u001b[38;5;241m=\u001b[39m dataset_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_count(dim)\n\u001b[0;32m--> 183\u001b[0m location \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gridstart_location_from_dim_index(dim, grid_idx[dim])\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_shape))\n\u001b[1;32m    186\u001b[0m ]\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(location)\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1698\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:636\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1113\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1091\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:496\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2197\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2194\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2197\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2199\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2266\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2263\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2264\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2266\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2267\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/localscratch/mamba/envs/splits/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for s in train_dset:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optional:*** inspect data configurations and <nobr>Micro$\\mathbb{S}$plit</nobr> config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_show_configs = True\n",
    "\n",
    "if do_show_configs:\n",
    "    print(\"FYI: train_data_config\")\n",
    "    print(\"----------------------\")\n",
    "    for cfg in train_data_config:\n",
    "        print(cfg)\n",
    "\n",
    "    print(\"\\nFYI: experiment_params\")\n",
    "    print(\"----------------------\")\n",
    "    print(experiment_params)\n",
    "else:\n",
    "    print(\"You opted out of having all params printed... swiftly moving on... ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: create Dataloaders for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.2:** Prepare <nobr>Micro$\\mathbb{S}$plit</nobr> Training\n",
    "Next, we create all the configs for the upcoming network training run. These lines are not very intuitive and if you don't intend to dive really deep into CAREamics and the internals of <nobr>Micro$\\mathbb{S}$plit</nobr>, you might just execute these cells and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our data_stas known to the experiment we prepare\n",
    "experiment_params[\"data_stats\"] = data_stats\n",
    "\n",
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the <nobr>Micro$\\mathbb{S}$plit</nobr> model to be trained.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load checkpoint (optional and for you to implement)*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> If you would like to continue a previous training run or finetune a compatible pre-trained model, here would be a good place. You will need to figure out how to implement this for your use-case, but to give you a head-start, we left three potentially useful lines of code below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from microsplit_reproducibility.notebook_utils.HT_LIF24 import load_pretrained_model\n",
    "# ckpt_path = load_checkpoint_path(f\"./pretrained_checkpoints/{EXPOSURE_DURATION}/\", best=True)\n",
    "# load_pretrained_model(model, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some training data for a final check!\n",
    "***Tip:*** we show you a few samples of the prepared training data. In case you don't like what you see, execute the cell again and other randomly chosen patches will be shown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_patches(dataset=train_dset, num_channels=NUM_CHANNELS, num_samples=3, patch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.3:** Train the prepared model!\n",
    "***Note:*** if this takes too long, there were to places above where we gave you options to *(i)* reduce the amount of training data, and *(ii)* chose to train for fewer epochs. Revisit your choices if you want to!\n",
    "\n",
    "***Note:*** Depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(\"./checkpoints/\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "# start the training\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.4:** Predict and visualize results for validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, reduce the validation dataset to speed up the evaluation\n",
    "val_dset.reduce_data([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note*** Parameter `mmse_count` is responsible for how many samples are generated for each patch. The default value is 1, but in this case you might see stitching artifacts because each patch will be slightly different. You can increase this value to 10 to get a smoother image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_test import (\n",
    "    get_unnormalized_predictions,\n",
    "    get_target,\n",
    "    get_input,\n",
    ")\n",
    "\n",
    "stitched_predictions, _, _ = get_unnormalized_predictions(\n",
    "    model, val_dset, mmse_count=1, num_workers=0, batch_size=8\n",
    ")\n",
    "tar = get_target(val_dset)\n",
    "\n",
    "# get input as sum of the two channels\n",
    "inp = get_input(val_dset).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: visualize predictions on validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "ax[0, 0].imshow(tar[0, ..., 0], cmap=\"gray\")\n",
    "ax[0, 0].set_title(\"Input ch1\")\n",
    "ax[0, 1].imshow(tar[0, ..., 1], cmap=\"gray\")\n",
    "ax[0, 1].set_title(\"Input ch2\")\n",
    "ax[1, 0].imshow(stitched_predictions[0, ..., 0], cmap=\"gray\")\n",
    "ax[1, 0].set_title(\"Prediction ch1\")\n",
    "ax[1, 1].imshow(stitched_predictions[0, ..., 1], cmap=\"gray\")\n",
    "ax[1, 1].set_title(\"Prediction ch2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_test import full_frame_evaluation\n",
    "\n",
    "frame_idx = 0\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds\"\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed view on some (foreground) locations...\n",
    "Below, we show few random foreground locations and the corresponding <nobr>Micro$\\mathbb{S}$plit</nobr> predictions.\n",
    "\n",
    "As before, also here you can execute the cell multiple times and different randomly chosen locations will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from microsplit_reproducibility.utils.utils import clean_ax\n",
    "from microsplit_reproducibility.notebook_utils.custom_test import (\n",
    "    pick_random_patches_with_content,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sz = 128\n",
    "rand_locations = pick_random_patches_with_content(tar, 128)\n",
    "h_start = rand_locations[\n",
    "    2, 0\n",
    "]  # np.random.randint(stitched_predictions.shape[1] - img_sz)\n",
    "w_start = rand_locations[\n",
    "    2, 1\n",
    "]  # np.random.randint(stitched_predictions.shape[2] - img_sz)\n",
    "\n",
    "ncols = 4 + 1\n",
    "nrows = min(len(rand_locations), 5)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "\n",
    "for i, (h_start, w_start) in enumerate(rand_locations[:nrows]):\n",
    "    ax[i, 0].imshow(inp[0, h_start : h_start + img_sz, w_start : w_start + img_sz])\n",
    "    for j in range(ncols // 2):\n",
    "        vmin = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].min()\n",
    "        vmax = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].max()\n",
    "        ax[i, 2 * j + 1].imshow(\n",
    "            tar[0, h_start : h_start + img_sz, w_start : w_start + img_sz, j],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        ax[i, 2 * j + 2].imshow(\n",
    "            stitched_predictions[\n",
    "                0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "            ],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "\n",
    "ax[0, 0].set_title(\"Primary Input\")\n",
    "for i in range(2):\n",
    "    ax[0, 2 * i + 1].set_title(f\"Target Channel {i+1}\")\n",
    "    ax[0, 2 * i + 2].set_title(f\"Predicted Channel {i+1}\")\n",
    "\n",
    "# reduce the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
    "clean_ax(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* manual inspection of the predictions\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Task:</b> Set <i>y_start</i>, <i>x_start</i>, and <i>crop_size</i> to inspect the predictions at a  location of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_start = 600  # np.random.randint(stitched_predictions.shape[1] - crop_size)\n",
    "x_start = 1150  # np.random.randint(stitched_predictions.shape[2] - crop_size)\n",
    "crop_size = 128\n",
    "\n",
    "ncols = 3\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "ax[0, 0].imshow(inp[0, y_start : y_start + crop_size, x_start : x_start + crop_size])\n",
    "for i in range(ncols - 1):\n",
    "    vmin = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].min()\n",
    "    vmax = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].max()\n",
    "    ax[0, i + 1].imshow(\n",
    "        tar[0, y_start : y_start + crop_size, x_start : x_start + crop_size, i],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    ax[1, i + 1].imshow(\n",
    "        stitched_predictions[\n",
    "            0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "        ],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "# disable the axis for ax[1,0]\n",
    "ax[1, 0].axis(\"off\")\n",
    "ax[0, 0].set_title(\"Input\")\n",
    "ax[0, 1].set_title(\"Channel 1\")\n",
    "ax[0, 2].set_title(\"Channel 2\")\n",
    "# set y labels on the right for ax[0,2]\n",
    "ax[0, 2].yaxis.set_label_position(\"right\")\n",
    "ax[0, 2].set_ylabel(\"Target\")\n",
    "\n",
    "ax[1, 2].yaxis.set_label_position(\"right\")\n",
    "ax[1, 2].set_ylabel(\"Predicted\")\n",
    "\n",
    "print(\"Here the crop you selected:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Optional Step 1.4:*** Posterior Sampling\n",
    "For a given input patch, <nobr>Micro$\\mathbb{S}$plit</nobr> can generate multiple outputs. This is possible because <nobr>Micro$\\mathbb{S}$plit</nobr> is learning a full posterior of possible solutions, which is a quite powerful feature!\n",
    "\n",
    "As we elaborate in the <nobr>Micro$\\mathbb{S}$plit</nobr> paper and also later in the calibration notebook `03_calibration.ipynb`, this allows users to visually judge and even quantify the (data) uncertainty in the predictions their trained model makes.\n",
    "\n",
    "Below, we show two posterior samples and how much they differ for a few random foreground locations. Re-run the cell to see different randomly choosen locations and corresponding posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_test import show_sampling\n",
    "\n",
    "imgsz = 3\n",
    "ncols = 6\n",
    "examplecount = 3\n",
    "_, ax = plt.subplots(\n",
    "    figsize=(imgsz * ncols, imgsz * 2 * examplecount),\n",
    "    ncols=ncols,\n",
    "    nrows=2 * examplecount,\n",
    ")\n",
    "\n",
    "show_sampling(val_dset, model, ax=ax[:2])\n",
    "show_sampling(val_dset, model, ax=ax[2:4])\n",
    "show_sampling(val_dset, model, ax=ax[4:6])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are done here! üëç Congratulations! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microsplit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
