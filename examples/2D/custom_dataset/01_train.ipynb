{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** Training a <nobr>Micro$\\mathbb{S}$plit</nobr> Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - what does this notebook do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will train a <nobr>Micro$\\mathbb{S}$plit</nobr> network for unmixing two superimposed channels for a custom dataset you provide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> How to organize your data for training a <nobr>Micro$\\mathbb{S}$plit</nobr> model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> We provide example data for this set of notebooks also, but the intention is to let a user use their own data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will train a <nobr>Micro$\\mathbb{S}$plit</nobr> network for unmixing two or more superimposed channels for a custom 2D dataset you provide. \n",
    "\n",
    "You should organize you dataset as follows:\n",
    "- Create a `data` directory\n",
    "- Create subdirectories `channel_1`, `channel_2`, etc, containing the channels you would like to unmix\n",
    "- Make sure that the images have the same spatial size and each image has only 1 channel\n",
    "\n",
    "Your data directory should look like this:\n",
    "```\n",
    "you_data_path/\n",
    "└── data\n",
    "    ├── channel_1\n",
    "    │   ├── image1.tiff\n",
    "    │   ├── image2.tiff\n",
    "    │   └── image3.tiff\n",
    "    └── channel_2\n",
    "    │   ├── image1.tiff\n",
    "    │   ├── image2.tiff\n",
    "    │   └── image3.tiff\n",
    "    └── channel_n\n",
    "    │   ├── image1.tiff\n",
    "    │   ├── image2.tiff\n",
    "    │   └── image3.tiff\n",
    "```\n",
    "\n",
    "The mixed image used for splitting will be obtained artificially by a convex combination of the target channels.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: what is <nobr>Micro$\\mathbb{S}$plit</nobr> training all about?\n",
    "Training is done in a supervised way. For every input patch, we have the two corresponding target patches using which we train our MicroSplit. \n",
    "Besides the primary input patch, we also feed LC inputs to MicroSplit. We introduced LC inputs in [μSplit: efficient image decomposition for microscopy data](https://openaccess.thecvf.com/content/ICCV2023/papers/Ashesh_uSplit_Image_Decomposition_for_Fluorescence_Microscopy_ICCV_2023_paper.pdf), which enabled the network to understand the global spatial context around the input patch.\n",
    "\n",
    "To enable unsupervised denoising, we integrated the KL loss formulation and Noise models from our previous work [denoiSplit: a method for joint microscopy image splitting and unsupervised denoising](https://eccv.ecva.net/virtual/2024/poster/2538). \n",
    "\n",
    "The loss function for MicroSplit is a weighted average of denoiSplit loss and μSplit loss. For both denoiSplit and μSplit, their loss expression have two terms: KL divergence loss and likelihood loss. For more details, please refer to the respective papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it, let's train a <nobr>Micro$\\mathbb{S}$plit</nobr> Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are new to Jupyter notebooks?** Don't worry, if you take the time to read all our explanations, we will guide you through them and you will understand a lot. Still, you will likely end up less frustrated, if you do not even start with the ambition to interpret the purpose of every line of code.\n",
    "Let's start with a nice example, the imports to enable the remainder of this notebook. Ignore it (unless you know what you are doing) and just click **⇧*Shift* + ⏎*Enter*** to execute this (and all other) code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things we need further down\n",
    "from pathlib import Path\n",
    "\n",
    "import pooch\n",
    "import matplotlib.pyplot as plt\n",
    "from careamics.lightning import VAEModule\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint_path\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.utils import plot_input_patches\n",
    "\n",
    "# Dataset specific imports...\n",
    "from microsplit_reproducibility.configs.parameters.custom_dataset_2D import (\n",
    "    get_microsplit_parameters\n",
    ")\n",
    "from microsplit_reproducibility.configs.data.custom_dataset_2D import get_data_configs\n",
    "from microsplit_reproducibility.datasets.custom_dataset_2D import get_train_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.1:** Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the channel unmixing capabilities of <nobr>Micro$\\mathbb{S}$plit</nobr> are trained in a supervised way, we must later feed *(i)* input images that contain both selected structures, and *(ii)* two seperate channels that show these two structures separately. As previosuly mentioned, the mixed input image is obtained synthetically by overlapping the other two channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pooch.create(\n",
    "    path=f\"./data/\",\n",
    "    base_url=f\"https://download.fht.org/jug/msplit/ht_lif24/data_tiff/\",\n",
    "    registry={f\"ht_lif24_5ms_reduced.zip\": None},\n",
    ")\n",
    "for fname in DATA.registry:\n",
    "    DATA.fetch(fname, processor=pooch.Unzip(), progressbar=True)\n",
    "\n",
    "DATA_PATH = DATA.abspath / (DATA.registry_files[0] + \".unzip/5ms/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR set the path to your own data\n",
    "Important: the path should end with `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = Path(\"/my/path/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the path to the noise models\n",
    "This is the path to the noise models that you trained in the notebook **00_noisemodels.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NM_PATH = Path(\"./noise_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we load the image data to be processed\n",
    "\n",
    "***Note*** that depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> below.\n",
    "\n",
    "Number of epochs is set to 10, which usually allows to see decent results. However, for getting optimal performance you can increase it to 50.\n",
    "\n",
    "You also need to change the `num_channels` and `target_channels` parameters respectively for the `get_data_configs()` and the `get_microsplit_parameters` functions. These parameters have a similar meaning, i.e., they control the number of channels in the input data depending on how many channels you want to split.\n",
    "\n",
    "Finally, ensure that the `image_size` parameter (i.e., the patch size in (`Z`, `Y`, `X`) you want to use to train the model) is properly set given the size of your data and of you GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 2\n",
    "\"\"\"The number of channels considered for the splitting task.\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "\"\"\"The batch size for training.\"\"\"\n",
    "PATCH_SIZE = (64, 64)\n",
    "\"\"\"The size of the patches fed to the network for training in (Y, X).\"\"\"\n",
    "EPOCHS = 10\n",
    "\"\"\"The number of epochs to train the network.\"\"\"\n",
    "\n",
    "assert len(PATCH_SIZE) == 2, \"PATCH_SIZE must be a tuple of length 2 (Y, X) since we are using 2D data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train, validation, and test data configs\n",
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    image_size=PATCH_SIZE,\n",
    "    num_channels=NUM_CHANNELS\n",
    ")\n",
    "\n",
    "# setting up MicroSplit parametrization\n",
    "experiment_params = get_microsplit_parameters(\n",
    "    algorithm=\"denoisplit\",\n",
    "    img_size=PATCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    multiscale_count=3,\n",
    "    noise_model_path=NM_PATH,\n",
    "    target_channels=NUM_CHANNELS,\n",
    ")\n",
    "\n",
    "# create the dataset\n",
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=DATA_PATH,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=get_train_val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optional:*** inspect data configurations and <nobr>Micro$\\mathbb{S}$plit</nobr> config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_show_configs = True\n",
    "\n",
    "if do_show_configs:\n",
    "    print(\"FYI: train_data_config\")\n",
    "    print(\"----------------------\")\n",
    "    for cfg in train_data_config:\n",
    "        print(cfg)\n",
    "\n",
    "    print(\"\\nFYI: experiment_params\")\n",
    "    print(\"----------------------\")\n",
    "    print(experiment_params)\n",
    "else:\n",
    "    print(\"You opted out of having all params printed... swiftly moving on... ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: create Dataloaders for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.2:** Prepare <nobr>Micro$\\mathbb{S}$plit</nobr> Training\n",
    "Next, we create all the configs for the upcoming network training run. These lines are not very intuitive and if you don't intend to dive really deep into CAREamics and the internals of <nobr>Micro$\\mathbb{S}$plit</nobr>, you might just execute these cells and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our data_stas known to the experiment we prepare\n",
    "experiment_params[\"data_stats\"] = data_stats\n",
    "\n",
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the <nobr>Micro$\\mathbb{S}$plit</nobr> model to be trained.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load checkpoint (optional and for you to implement)*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> If you would like to continue a previous training run or finetune a compatible pre-trained model, here would be a good place. You will need to figure out how to implement this for your use-case, but to give you a head-start, we left three potentially useful lines of code below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from microsplit_reproducibility.notebook_utils.HT_LIF24 import load_pretrained_model\n",
    "# ckpt_path = load_checkpoint_path(f\"./pretrained_checkpoints/{EXPOSURE_DURATION}/\", best=True)\n",
    "# load_pretrained_model(model, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some training data for a final check!\n",
    "***Tip:*** we show you a few samples of the prepared training data. In case you don't like what you see, execute the cell again and other randomly chosen patches will be shown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_patches(dataset=train_dset, num_channels=NUM_CHANNELS, num_samples=3, patch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.3:** Train the prepared model!\n",
    "***Note:*** if this takes too long, there were to places above where we gave you options to *(i)* reduce the amount of training data, and *(ii)* chose to train for fewer epochs. Revisit your choices if you want to!\n",
    "\n",
    "***Note:*** Depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(\"./checkpoints/\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "# start the training\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.4:** Predict and visualize results for validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, reduce the validation dataset to speed up the evaluation\n",
    "val_dset.reduce_data([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note*** Parameter `mmse_count` is responsible for how many samples are generated for each patch. The default value is 1, but in this case you might see stitching artifacts because each patch will be slightly different. You can increase this value to 10 to get a smoother image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import (\n",
    "    get_unnormalized_predictions,\n",
    "    get_target,\n",
    "    get_input,\n",
    ")\n",
    "\n",
    "stitched_predictions, _, _ = get_unnormalized_predictions(\n",
    "    model, val_dset, mmse_count=experiment_params['mmse_count'], num_workers=0, batch_size=8\n",
    ")\n",
    "tar = get_target(val_dset)\n",
    "\n",
    "# get input as sum of the two channels\n",
    "inp = get_input(val_dset).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: visualize predictions on validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "ax[0, 0].imshow(tar[0, ..., 0], cmap=\"gray\")\n",
    "ax[0, 0].set_title(\"Input ch1\")\n",
    "ax[0, 1].imshow(tar[0, ..., 1], cmap=\"gray\")\n",
    "ax[0, 1].set_title(\"Input ch2\")\n",
    "ax[1, 0].imshow(stitched_predictions[0, ..., 0], cmap=\"gray\")\n",
    "ax[1, 0].set_title(\"Prediction ch1\")\n",
    "ax[1, 1].imshow(stitched_predictions[0, ..., 1], cmap=\"gray\")\n",
    "ax[1, 1].set_title(\"Prediction ch2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import full_frame_evaluation\n",
    "\n",
    "frame_idx = 0\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds\"\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed view on some (foreground) locations...\n",
    "Below, we show few random foreground locations and the corresponding <nobr>Micro$\\mathbb{S}$plit</nobr> predictions.\n",
    "\n",
    "As before, also here you can execute the cell multiple times and different randomly chosen locations will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from microsplit_reproducibility.utils.utils import clean_ax\n",
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import (\n",
    "    pick_random_patches_with_content,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sz = 128\n",
    "rand_locations = pick_random_patches_with_content(tar, 128)\n",
    "h_start = rand_locations[\n",
    "    2, 0\n",
    "]  # np.random.randint(stitched_predictions.shape[1] - img_sz)\n",
    "w_start = rand_locations[\n",
    "    2, 1\n",
    "]  # np.random.randint(stitched_predictions.shape[2] - img_sz)\n",
    "\n",
    "ncols = 4 + 1\n",
    "nrows = min(len(rand_locations), 5)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "\n",
    "for i, (h_start, w_start) in enumerate(rand_locations[:nrows]):\n",
    "    ax[i, 0].imshow(inp[0, h_start : h_start + img_sz, w_start : w_start + img_sz])\n",
    "    for j in range(ncols // 2):\n",
    "        vmin = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].min()\n",
    "        vmax = stitched_predictions[\n",
    "            0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "        ].max()\n",
    "        ax[i, 2 * j + 1].imshow(\n",
    "            tar[0, h_start : h_start + img_sz, w_start : w_start + img_sz, j],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        ax[i, 2 * j + 2].imshow(\n",
    "            stitched_predictions[\n",
    "                0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "            ],\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "\n",
    "ax[0, 0].set_title(\"Primary Input\")\n",
    "for i in range(2):\n",
    "    ax[0, 2 * i + 1].set_title(f\"Target Channel {i+1}\")\n",
    "    ax[0, 2 * i + 2].set_title(f\"Predicted Channel {i+1}\")\n",
    "\n",
    "# reduce the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
    "clean_ax(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* manual inspection of the predictions\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Task:</b> Set <i>y_start</i>, <i>x_start</i>, and <i>crop_size</i> to inspect the predictions at a  location of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_start = 600  # np.random.randint(stitched_predictions.shape[1] - crop_size)\n",
    "x_start = 1150  # np.random.randint(stitched_predictions.shape[2] - crop_size)\n",
    "crop_size = 128\n",
    "\n",
    "ncols = 3\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "ax[0, 0].imshow(inp[0, y_start : y_start + crop_size, x_start : x_start + crop_size])\n",
    "for i in range(ncols - 1):\n",
    "    vmin = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].min()\n",
    "    vmax = stitched_predictions[\n",
    "        0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "    ].max()\n",
    "    ax[0, i + 1].imshow(\n",
    "        tar[0, y_start : y_start + crop_size, x_start : x_start + crop_size, i],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    ax[1, i + 1].imshow(\n",
    "        stitched_predictions[\n",
    "            0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "        ],\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "# disable the axis for ax[1,0]\n",
    "ax[1, 0].axis(\"off\")\n",
    "ax[0, 0].set_title(\"Input\")\n",
    "ax[0, 1].set_title(\"Channel 1\")\n",
    "ax[0, 2].set_title(\"Channel 2\")\n",
    "# set y labels on the right for ax[0,2]\n",
    "ax[0, 2].yaxis.set_label_position(\"right\")\n",
    "ax[0, 2].set_ylabel(\"Target\")\n",
    "\n",
    "ax[1, 2].yaxis.set_label_position(\"right\")\n",
    "ax[1, 2].set_ylabel(\"Predicted\")\n",
    "\n",
    "print(\"Here the crop you selected:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Optional Step 1.4:*** Posterior Sampling\n",
    "For a given input patch, <nobr>Micro$\\mathbb{S}$plit</nobr> can generate multiple outputs. This is possible because <nobr>Micro$\\mathbb{S}$plit</nobr> is learning a full posterior of possible solutions, which is a quite powerful feature!\n",
    "\n",
    "As we elaborate in the <nobr>Micro$\\mathbb{S}$plit</nobr> paper and also later in the calibration notebook `03_calibration.ipynb`, this allows users to visually judge and even quantify the (data) uncertainty in the predictions their trained model makes.\n",
    "\n",
    "Below, we show two posterior samples and how much they differ for a few random foreground locations. Re-run the cell to see different randomly choosen locations and corresponding posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import show_sampling\n",
    "\n",
    "imgsz = 3\n",
    "ncols = 6\n",
    "examplecount = 3\n",
    "_, ax = plt.subplots(\n",
    "    figsize=(imgsz * ncols, imgsz * 2 * examplecount),\n",
    "    ncols=ncols,\n",
    "    nrows=2 * examplecount,\n",
    ")\n",
    "\n",
    "show_sampling(val_dset, model, ax=ax[:2])\n",
    "show_sampling(val_dset, model, ax=ax[2:4])\n",
    "show_sampling(val_dset, model, ax=ax[4:6])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are done here! 👍 Congratulations! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
