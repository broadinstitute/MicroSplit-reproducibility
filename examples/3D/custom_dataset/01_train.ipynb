{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** Training a <nobr>Micro$\\mathbb{S}$plit</nobr> Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - what does this notebook do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Despite network training arguably being the most important step, the execution of this notebook is optional. If you do not run this notebook, in Step 2 (prediction) we offer you to use pretrained model checkpoints. Like in any respected cooking show: \"we have already prepared someting before the show\"... üòâ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will train a <nobr>Micro$\\mathbb{S}$plit</nobr> network for unmixing two or more superimposed channels for a custom 3D dataset you provide. \n",
    "\n",
    "You should organize you dataset as follows:\n",
    "- Create a `data` directory\n",
    "- Create subdirectories `channel_1`, `channel_2`, etc, containing the channels you would like to unmix\n",
    "- Make sure that the images have the same spatial size and each image has only 1 channel\n",
    "\n",
    "Your data directory should look like this:\n",
    "```\n",
    "you_data_path/\n",
    "‚îî‚îÄ‚îÄ data\n",
    "    ‚îú‚îÄ‚îÄ channel_1\n",
    "    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ image1.tiff\n",
    "    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ image2.tiff\n",
    "    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ image3.tiff\n",
    "    ‚îî‚îÄ‚îÄ channel_2\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image1.tiff\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image2.tiff\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ image3.tiff\n",
    "    ‚îî‚îÄ‚îÄ channel_n\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image1.tiff\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ image2.tiff\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ image3.tiff\n",
    "```\n",
    "\n",
    "The mixed image used for splitting will be obtained artificially by a convex combination of the target channels.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: what is <nobr>Micro$\\mathbb{S}$plit</nobr> training all about?\n",
    "Training is done in a supervised way. For every input patch, we have the corresponding target patches using which we train our MicroSplit. \n",
    "Besides the primary input patch, we also feed LC inputs to MicroSplit. We introduced LC inputs in [ŒºSplit: efficient image decomposition for microscopy data](https://openaccess.thecvf.com/content/ICCV2023/papers/Ashesh_uSplit_Image_Decomposition_for_Fluorescence_Microscopy_ICCV_2023_paper.pdf), which enabled the network to understand the global spatial context around the input patch.\n",
    "\n",
    "To enable unsupervised denoising, we integrated the KL loss formulation and Noise models from our previous work [denoiSplit: a method for joint microscopy image splitting and unsupervised denoising](https://eccv.ecva.net/virtual/2024/poster/2538). \n",
    "\n",
    "The loss function for MicroSplit is a weighted average of denoiSplit loss and ŒºSplit loss. For both denoiSplit and ŒºSplit, their loss expression have two terms: KL divergence loss and likelihood loss. For more details, please refer to the respective papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it, let's train a <nobr>Micro$\\mathbb{S}$plit</nobr> Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are new to Jupyter notebooks?** Don't worry, if you take the time to read all our explanations, we will guide you through them and you will understand a lot. Still, you will likely end up less frustrated, if you do not even start with the ambition to interpret the purpose of every line of code.\n",
    "Let's start with a nice example, the imports to enable the remainder of this notebook. Ignore it (unless you know what you are doing) and just click **‚áß*Shift* + ‚èé*Enter*** to execute this (and all other) code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things we need further down\n",
    "\n",
    "import pooch\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from careamics.lightning import VAEModule\n",
    "\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint, load_checkpoint_path\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.utils import (\n",
    "    plot_training_metrics,\n",
    "    plot_input_patches_3d,\n",
    "    plot_training_outputs,\n",
    ")\n",
    "\n",
    "# Dataset specific imports...\n",
    "from microsplit_reproducibility.configs.parameters.custom_dataset_3D import get_microsplit_parameters\n",
    "from microsplit_reproducibility.configs.data.custom_dataset_3D import get_data_configs\n",
    "from microsplit_reproducibility.datasets.custom_dataset_3D import get_train_val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.1:** Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pooch.create(\n",
    "    path=f\"./data/\",\n",
    "    base_url=f\"https://download.fht.org/jug/msplit/ht_lif24/data_tiff/\",\n",
    "    registry={f\"ht_lif24_5ms_reduced.zip\": None},\n",
    ")\n",
    "for fname in DATA.registry:\n",
    "    DATA.fetch(fname, processor=pooch.Unzip(), progressbar=True)\n",
    "\n",
    "DATA_PATH = DATA.abspath / (DATA.registry_files[0] + \".unzip/5ms/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR set the path to your own data\n",
    "Important: the path should end with `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = Path(\"path/to/my/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the path to the noise models\n",
    "This is the path to the noise models that you trained in the notebook **00_noisemodels.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NM_PATH = Path(\"./noise_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we load the image data to be processed\n",
    "\n",
    "Note that depending on the amount of GPU memory you have available, you might want to adjust the `BATCH_SIZE`. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter.\n",
    "\n",
    "Number of `EPOCHS` is set to 3, which usually allows to see decent results for 3D datasets. However, for getting optimal performance you can increase it to 10.\n",
    "\n",
    "You also need to specify:\n",
    "- the `NUM_CHANNELS` parameter, which controls the number of channels in the input data depending on how many channels you want to split.\n",
    "- the `NUM_Z_SLICES` parameter, which tells the number of slices in your Z-stack. **NOTE: we expect all your input images to have the same number of Z-slices!!!**\n",
    "\n",
    "Finally, ensure that the `PATCH_SIZE` parameter (i.e., the patch size in (`Z`, `Y`, `X`) you want to use to train the model) is properly set given the size of your data and of you GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 2\n",
    "\"\"\"The number of channels considered for the splitting task.\"\"\"\n",
    "NUM_Z_SLICES = 20\n",
    "\"\"\"The number of z slices in the input data.\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "\"\"\"The batch size for training.\"\"\"\n",
    "PATCH_SIZE = (8, 64, 64)\n",
    "\"\"\"The size of the patches fed to the network for training in (Z, Y, X).\"\"\"\n",
    "EPOCHS = 3\n",
    "\"\"\"The number of epochs to train the network.\"\"\"\n",
    "\n",
    "assert len(PATCH_SIZE) == 3, \"Patch size must be a tuple of length 3 (Z, Y, X) since we are using 3D data.\"\n",
    "assert PATCH_SIZE[0] <= NUM_Z_SLICES, \"Patch size in Z dimension must be smaller than or equal to the number of z slices in the input data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train, validation, and test data configs\n",
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    image_size=PATCH_SIZE,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    num_z_slices=NUM_Z_SLICES,\n",
    ")\n",
    "\n",
    "# setting up MicroSplit parametrization\n",
    "experiment_params = get_microsplit_parameters(\n",
    "    algorithm=\"denoisplit\",\n",
    "    img_size=PATCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    multiscale_count=1,\n",
    "    noise_model_path=NM_PATH,\n",
    "    target_channels=NUM_CHANNELS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the download of required files\n",
    "train_dset, val_dset, _, data_stats = create_train_val_datasets(\n",
    "    datapath=DATA_PATH,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=get_train_val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset._data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optional:*** inspect data configurations and <nobr>Micro$\\mathbb{S}$plit</nobr> config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_show_configs = True\n",
    "\n",
    "if do_show_configs:\n",
    "    print('FYI: train_data_config')\n",
    "    print('----------------------')\n",
    "    for cfg in train_data_config:\n",
    "        print(cfg)\n",
    "\n",
    "    print('\\nFYI: experiment_params')\n",
    "    print('----------------------')\n",
    "    print(experiment_params)\n",
    "else:\n",
    "    print('You opted out of having all params printed... swiftly moving on... ;)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wanna trade speed for model quality?\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you just want to get an idea of the process of training <nobr>Micro$\\mathbb{S}$plit</nobr> and you do not intend to get best-possible results, feel invited to crop down on the training data to be used further down. <i><b>Do not do this</b> if you intend to train a competitive model!!!</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, training and validation data will be reduced to only \n",
    "# consisting of 2 and 1 frames, respectively.\n",
    "reduce_data = False\n",
    "\n",
    "if reduce_data:\n",
    "    print(\"Using REDUCED training and validation data for quick'n'dirty testing!\")\n",
    "    train_dset.reduce_data([0, 1])\n",
    "    val_dset.reduce_data([0])\n",
    "else:\n",
    "    print('Using the full set of training and validation data!')\n",
    "print(f'(This are {train_dset.get_num_frames()} and {val_dset.get_num_frames()} frames, respectively.)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: create Dataloaders for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=experiment_params[\"batch_size\"],\n",
    "    num_workers=experiment_params[\"num_workers\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.2:** Prepare <nobr>Micro$\\mathbb{S}$plit</nobr> Training\n",
    "Next, we create all the configs for the upcoming network training run. These lines are not very intuitive and if you don't intend to dive really deep into CAREamics and the internals of <nobr>Micro$\\mathbb{S}$plit</nobr>, you might just execute these cells and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our data_stas known to the experiment we prepare\n",
    "experiment_params[\"data_stats\"] = data_stats\n",
    "\n",
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the <nobr>Micro$\\mathbb{S}$plit</nobr> model to be trained.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load checkpoint (optional and for you to implement)*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> If you would like to continue a previous training run or finetune a compatible pre-trained model, here would be a good place. You will need to figure out how to implement this for your use-case, but to give you a head-start, we left three potentially useful lines of code below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_3D import load_pretrained_model\n",
    "\n",
    "ckpt_path = load_checkpoint_path(f\"./checkpoints/\", best=True)\n",
    "load_pretrained_model(model, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some training data for a final check!\n",
    "***Tip:*** we show you a few samples of the prepared training data. In case you don't like what you see, execute the cell again and other randomly chosen patches will be shown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_patches_3d(dataset=train_dset, num_channels=NUM_CHANNELS, num_samples=3, patch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.3:** Train the prepared model!\n",
    "***Note:*** if this takes too long, there were to places above where we gave you options to *(i)* reduce the amount of training data, and *(ii)* chose to train for fewer epochs. Revisit your choices if you want to!\n",
    "\n",
    "***Note:*** Depending on the amount of GPU memory you have available, you might want to adjust the batch size. The default is 32, but you can reduce it to 16 if you run out of memory by changing the <i> batch_size </i> parameter in <i> get_microsplit_parameters </i> above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CAREamics 'Trainer'\n",
    "trainer = Trainer(\n",
    "    max_epochs=training_config.num_epochs,\n",
    "    # NOTE: if you are on a mac swap the accelerator to \"mps\"\n",
    "    # accelerator=‚Äúmps‚Äù,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(f\"./checkpoints/\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    ")\n",
    "\n",
    "# start the training - yay!\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training loss curves...\n",
    "Below, we plot for each epoch of your training run the *(i)* training reconstruction loss, *(ii)* training KL divergence loss, *(iii)* validation reconstruction loss, and *(iv)* validation PSNR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import find_recent_metrics, plot_metrics\n",
    "\n",
    "df = read_csv(find_recent_metrics())\n",
    "plot_metrics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.4:** Predict and visualize results for validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_3D import (\n",
    "    get_unnormalized_predictions, get_target, get_input\n",
    ")\n",
    "\n",
    "stitched_predictions, _, _ = get_unnormalized_predictions(\n",
    "    model,\n",
    "    val_dset,\n",
    "    data_key=str(DATA_PATH), # FIXME: check if this works for different cases\n",
    "    # mmse_count=experiment_params['mmse_count'],\n",
    "    mmse_count=2,\n",
    "    num_workers=0,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = get_target(val_dset)\n",
    "inp = get_input(val_dset).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape, inp.shape, stitched_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: visualize predictions on validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.custom_dataset_3D import full_frame_evaluation\n",
    "\n",
    "frame_idx = 0\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds\"\n",
    "\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed view on some (foreground) locations...\n",
    "Below, we show few random foreground locations and the corresponding <nobr>Micro$\\mathbb{S}$plit</nobr> predictions.\n",
    "\n",
    "As before, also here you can execute the cell multiple times and different randomly chosen locations will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.utils.utils import clean_ax\n",
    "from microsplit_reproducibility.notebook_utils.custom_dataset_3D import pick_random_patches_with_content\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sz = 128\n",
    "rand_locations = pick_random_patches_with_content(tar, 128)\n",
    "h_start = rand_locations[2, 1] #np.random.randint(stitched_predictions.shape[1] - img_sz)\n",
    "w_start = rand_locations[2, 2] #np.random.randint(stitched_predictions.shape[2] - img_sz)\n",
    "\n",
    "ncols = 2*NUM_CHANNELS + 1\n",
    "nrows = min(len(rand_locations), 5)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "\n",
    "for i, (z_idx, h_start, w_start) in enumerate(rand_locations[:nrows]):\n",
    "    ax[i, 0].imshow(inp[0, z_idx, h_start:h_start+img_sz, w_start:w_start+img_sz])\n",
    "    for j in range(ncols//2):\n",
    "        vmin = stitched_predictions[0, z_idx, h_start:h_start+img_sz, w_start:w_start+img_sz, j].min()\n",
    "        vmax = stitched_predictions[0, z_idx, h_start:h_start+img_sz, w_start:w_start+img_sz, j].max()\n",
    "        ax[i, 2*j+1].imshow(tar[0, z_idx, h_start:h_start+img_sz, w_start:w_start+img_sz, j], vmin=vmin, vmax=vmax)\n",
    "        ax[i, 2*j+2].imshow(stitched_predictions[0, z_idx, h_start:h_start+img_sz, w_start:w_start+img_sz, j], vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax[0,0].set_title('Primary Input')\n",
    "for i in range(NUM_CHANNELS):\n",
    "    ax[0, 2*i+1].set_title(f'Target Channel {i+1}')\n",
    "    ax[0, 2*i+2].set_title(f'Predicted Channel {i+1}')\n",
    "\n",
    "# reduce the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
    "clean_ax(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* manual inspection of the predictions\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Task:</b> Set <i>y_start</i>, <i>x_start</i>, and <i>crop_size</i> to inspect the predictions at a  location of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_start = 600  #np.random.randint(stitched_predictions.shape[1] - crop_size)\n",
    "x_start = 1150 #np.random.randint(stitched_predictions.shape[2] - crop_size)\n",
    "z_idx = 0\n",
    "crop_size = 128\n",
    "\n",
    "ncols = NUM_CHANNELS + 1\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "ax[0,0].imshow(inp[0, z_idx, y_start:y_start+crop_size, x_start:x_start+crop_size])\n",
    "for i in range(ncols -1):\n",
    "    vmin = stitched_predictions[0,z_idx, y_start:y_start+crop_size, x_start:x_start+crop_size, i].min()\n",
    "    vmax = stitched_predictions[0,z_idx, y_start:y_start+crop_size, x_start:x_start+crop_size, i].max()\n",
    "    ax[0,i+1].imshow(tar[0,z_idx, y_start:y_start+crop_size, x_start:x_start+crop_size, i], vmin=vmin, vmax=vmax)\n",
    "    ax[1,i+1].imshow(stitched_predictions[0, z_idx, y_start:y_start+crop_size, x_start:x_start+crop_size, i], vmin=vmin, vmax=vmax)\n",
    "\n",
    "# disable the axis for ax[1,0]\n",
    "ax[1,0].axis('off')\n",
    "ax[0,0].set_title(\"Input\")\n",
    "ax[0,1].set_title(\"Channel 1\")\n",
    "ax[0,2].set_title(\"Channel 2\")\n",
    "# set y labels on the right for ax[0,2]\n",
    "ax[0,2].yaxis.set_label_position(\"right\")\n",
    "ax[0,2].set_ylabel(\"Target\")\n",
    "\n",
    "ax[1,2].yaxis.set_label_position(\"right\")\n",
    "ax[1,2].set_ylabel(\"Predicted\")\n",
    "\n",
    "print('Here the crop you selected:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Optional Step 1.4:*** Posterior Sampling\n",
    "For a given input patch, <nobr>Micro$\\mathbb{S}$plit</nobr> can generate multiple outputs. This is possible because <nobr>Micro$\\mathbb{S}$plit</nobr> is learning a full posterior of possible solutions, which is a quite powerful feature!\n",
    "\n",
    "As we elaborate in the <nobr>Micro$\\mathbb{S}$plit</nobr> paper and also later in the calibration notebook `03_calibration.ipynb`, this allows users to visually judge and even quantify the (data) uncertainty in the predictions their trained model makes.\n",
    "\n",
    "Below, we show two posterior samples and how much they differ for a few random foreground locations. Re-run the cell to see different randomly choosen locations and corresponding posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from microsplit_reproducibility.notebook_utils.HT_LIF24 import show_sampling\n",
    "imgsz = 3\n",
    "ncols = 6\n",
    "examplecount = 3\n",
    "_,ax = plt.subplots(figsize=(imgsz*ncols, imgsz*2*examplecount), ncols=ncols, nrows=2*examplecount)\n",
    "\n",
    "show_sampling(val_dset, model, ax=ax[:2])\n",
    "show_sampling(val_dset, model, ax=ax[2:4])\n",
    "show_sampling(val_dset, model, ax=ax[4:6])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are done here! üëç Congratulations! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microsplit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
